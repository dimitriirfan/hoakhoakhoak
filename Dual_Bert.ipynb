{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dual Bert",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "veulxrjaRaVT",
        "outputId": "0bb8a3c4-cbd3-4310-bf81-8b621f1c435c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUKy3WguRqxX",
        "outputId": "9052b3b2-63d5-4f0b-bfb3-e4d53d2f5365",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# generics \n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "\n",
        "!pip install pytypo\n",
        "import pytypo\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import BertTokenizer, AutoModel, BertConfig, TFBertModel, AdamW, get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup, BertConfig, get_constant_schedule_with_warmup\n",
        "\n",
        "# import warnings\n",
        "# warnings.filterwarnings('FutureWarning')\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytypo in /usr/local/lib/python3.6/dist-packages (0.3.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad4y5HFeRsJH",
        "outputId": "9179c124-0e6b-4903-ea35-6ce620adae02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSMT8mbYlmw5"
      },
      "source": [
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "set_seed(29092020)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oa9uNO33YH5"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
        "model = AutoModel.from_pretrained(\"indobenchmark/indobert-base-p1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY4-Ketnvcvb"
      },
      "source": [
        "# df = pd.read_excel('./drive/My Drive/satdat/dataset.xlsx')\n",
        "df_train = pd.read_csv('./drive/My Drive/satdat/train.csv')\n",
        "df_val = pd.read_csv('./drive/My Drive/satdat/val.csv')\n",
        "test = pd.read_csv('./drive/My Drive/satdat/datatest_labelled.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1euaxNfvn1RI"
      },
      "source": [
        "# df_train, df_val = train_test_split(df, test_size=0.1, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM2FFRN63bFc"
      },
      "source": [
        "# df_train.to_csv(\"./drive/My Drive/satdat/b_train.csv\")\n",
        "# df_val.to_csv('./drive/My Drive/satdat/b_val.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqGdZISp4FDG"
      },
      "source": [
        "def clean(text) : \n",
        "\n",
        "  text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[#]+|[^A-Za-z0-9]+\"\n",
        "  text_cleaning_hash = \"#[A-Za-z0-9]+\" \n",
        "  text_cleaning_num = \"(^|\\W)\\d+\"\n",
        "\n",
        "  text = re.sub(text_cleaning_hash, \" \", text).strip()\n",
        "  text = re.sub(text_cleaning_num, \" \", text).strip()\n",
        "  text = re.sub(text_cleaning_re, \" \", text).strip()\n",
        "  \n",
        "  text = text.strip()\n",
        "\n",
        "  out = []\n",
        "  for word in text.split() :\n",
        "    # try : \n",
        "    #   out.append(word.replace(word, slang[word]))\n",
        "    # except Exception as e : \n",
        "    out.append(word)\n",
        "      \n",
        "  return pytypo.correct_sentence(\" \".join(out).strip())\n",
        "\n",
        "slang = pd.read_csv('./drive/My Drive/satdat/slang.csv')\n",
        "slang = slang[['slang', 'formal']]\n",
        "slang = slang.set_index('slang')['formal'].to_dict()\n",
        "\n",
        "\n",
        "df_train.narasi = df_train.narasi.apply(lambda x: clean(x)) \n",
        "df_train.judul = df_train.judul.apply(lambda x: clean(x))\n",
        "\n",
        "df_val.narasi = df_val.narasi.apply(lambda x: clean(x)) \n",
        "df_val.judul = df_val.judul.apply(lambda x: clean(x))\n",
        "\n",
        "test.narasi = test.narasi.apply(lambda x: clean(x))\n",
        "test.judul = test.judul.apply(lambda x: clean(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whSqerSlRzEG"
      },
      "source": [
        "class HoaxDataset(Dataset) : \n",
        "  def __init__(self, feature1, feature2, label, tokenizer, max_len, no_label=False) : \n",
        "    self.feature1 = feature1\n",
        "    self.feature2 = feature2\n",
        "    self.label = label \n",
        "    self.tokenizer = tokenizer \n",
        "    self.max_len = max_len \n",
        "    self.no_label = no_label\n",
        "\n",
        "  def __len__(self) : \n",
        "    return len(self.feature1)\n",
        "  \n",
        "  def __getitem__(self, item) :\n",
        "    feature1 = str(self.feature1[item])\n",
        "    feature2 = str(self.feature2[item])\n",
        "    if not self.no_label: \n",
        "      label = self.label[item]\n",
        "\n",
        "    encoding1 = tokenizer.encode_plus(\n",
        "        # ntar diganti <----------------------------------------------------\n",
        "        feature1, \n",
        "        max_length=64,\n",
        "        add_special_tokens=True,\n",
        "        return_token_type_ids=False,\n",
        "        return_attention_mask=True,\n",
        "        truncation=True, \n",
        "        pad_to_max_length=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    encoding2 = tokenizer.encode_plus(\n",
        "        feature2, \n",
        "        max_length=32,\n",
        "        add_special_tokens=True,\n",
        "        return_token_type_ids=False,\n",
        "        return_attention_mask=True,\n",
        "        truncation=True, \n",
        "        pad_to_max_length=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    \n",
        "    if not self.no_label :\n",
        "      return {\n",
        "          'narasi_text' : feature1,\n",
        "          'narasi_input_ids' : encoding1['input_ids'].flatten(), \n",
        "          'narasi_attention_mask' : encoding1['attention_mask'].flatten(), \n",
        "\n",
        "          'judul_narasi_text' : feature2,\n",
        "          'judul_input_ids' : encoding2['input_ids'].flatten(), \n",
        "          'judul_attention_mask' : encoding2['attention_mask'].flatten(), \n",
        "\n",
        "          'label' : torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "      }\n",
        "    else : \n",
        "      return {\n",
        "          'narasi_text' : feature1,\n",
        "          'narasi_input_ids' : encoding1['input_ids'].flatten(), \n",
        "          'narasi_attention_mask' : encoding1['attention_mask'].flatten(), \n",
        "\n",
        "          'judul_narasi_text' : feature2,\n",
        "          'judul_input_ids' : encoding2['input_ids'].flatten(), \n",
        "          'judul_attention_mask' : encoding2['attention_mask'].flatten(), \n",
        "\n",
        "      }\n",
        "\n",
        "\n",
        "def to_data_loader(df, columns, label, tokenizer, max_len, batch_size) : \n",
        "  ds = HoaxDataset(\n",
        "      df[columns[0]], \n",
        "      df[columns[1]], \n",
        "      df[label],\n",
        "      tokenizer=tokenizer, \n",
        "      max_len=max_len, \n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "      ds, \n",
        "      batch_size=batch_size, \n",
        "  )\n",
        "\n",
        "def test_to_data_loader(df, columns, tokenizer, max_len, batch_size) : \n",
        "  ds = HoaxDataset(\n",
        "      df[columns[0]], \n",
        "      df[columns[1]], \n",
        "      None,\n",
        "      tokenizer=tokenizer, \n",
        "      max_len=max_len, \n",
        "      no_label=True\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "      ds, \n",
        "      batch_size=batch_size, \n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cuSe1rRWdTp"
      },
      "source": [
        "train_data_loader = to_data_loader(df_train, ['narasi', 'judul'], 'label', tokenizer, 64, 32)\n",
        "val_data_loader = to_data_loader(df_val, ['narasi', 'judul'], 'label', tokenizer, 64, 32)\n",
        "test_data_loader = test_to_data_loader(test, ['narasi', 'judul'], tokenizer, 64, 32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqMS6aFhX42i",
        "outputId": "50693afa-36af-4c8a-acac-73433485773c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "data = next(iter(test_data_loader))\n",
        "data.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1773: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['narasi_text', 'narasi_input_ids', 'narasi_attention_mask', 'judul_narasi_text', 'judul_input_ids', 'judul_attention_mask'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-rlTZksR1m1"
      },
      "source": [
        "class HoaxClassifier(nn.Module) : \n",
        "  def __init__(self, n_classes) : \n",
        "    super(HoaxClassifier, self).__init__() \n",
        "    config = BertConfig.from_pretrained('indobenchmark/indobert-base-p1')\n",
        "\n",
        "    self.bert1 = AutoModel.from_pretrained(\"indobenchmark/indobert-base-p1\", config=config)\n",
        "    self.bert2 = AutoModel.from_pretrained(\"indobenchmark/indobert-base-p1\", config=config)\n",
        "\n",
        "    self.drop = nn.Dropout(p=0.5)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.tanh = nn.Tanh()\n",
        "\n",
        "    self.dual_bert = nn.Linear(self.bert1.config.hidden_size * 2, 32)\n",
        "\n",
        "    self.out = nn.Linear(32, 2) \n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, narasi_input_ids, narasi_attention_mask, judul_input_ids, judul_attention_mask) :\n",
        "    _, pooled_output1 = self.bert1(\n",
        "        input_ids = narasi_input_ids, \n",
        "        attention_mask = narasi_attention_mask\n",
        "    )\n",
        "\n",
        "    _, pooled_output2 = self.bert2(\n",
        "        input_ids = judul_input_ids, \n",
        "        attention_mask = judul_attention_mask\n",
        "    )\n",
        "\n",
        "    x = torch.cat((pooled_output1, pooled_output2), dim=1)\n",
        "\n",
        "    x = self.drop(x)\n",
        "    x = self.dual_bert(x)\n",
        "    x = self.tanh(x) \n",
        "\n",
        "    x = self.drop(x)\n",
        "    x = self.out(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvFNzJ5_zbcQ",
        "outputId": "7c12a27b-30b2-47a5-9d7a-600d4d092a72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = HoaxClassifier(2)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HoaxClassifier(\n",
              "  (bert1): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(50000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (bert2): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(50000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (drop): Dropout(p=0.5, inplace=False)\n",
              "  (relu): ReLU()\n",
              "  (tanh): Tanh()\n",
              "  (dual_bert): Linear(in_features=1536, out_features=32, bias=True)\n",
              "  (out): Linear(in_features=32, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRRpXTg1Sr_h"
      },
      "source": [
        "# load freezed only if already exist\n",
        "# model.load_state_dict(torch.load('/content/drive/My Drive/satdat/freezed_state.bin'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_koU2e1u3r6"
      },
      "source": [
        "# toggle to train non embeddings\n",
        "model.bert1.embeddings.requires_grad_=True\n",
        "model.bert2.embeddings.requires_grad_=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHa_UqnSR7GL"
      },
      "source": [
        "EPOCHS = 8\n",
        "opt = AdamW(model.parameters(), lr=3e-5, correct_bias=False, weight_decay=1e-4)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_constant_schedule_with_warmup(\n",
        "    opt, \n",
        "    num_warmup_steps=0, \n",
        "    # num_training_steps=total_steps, \n",
        "\n",
        ")\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jEkqYslR8TH"
      },
      "source": [
        "def train_epoch (model, data_loader, loss_fn, optimizer, device, scheduler, n_examples) : \n",
        "\n",
        "  model = model.train() \n",
        "\n",
        "  correct_predictions = 0\n",
        "  losses = []\n",
        "\n",
        "  for d in data_loader : \n",
        "    input_ids1 = d['narasi_input_ids'].to(device)\n",
        "    input_ids2 = d['judul_input_ids'].to(device)\n",
        "\n",
        "    input_mask1 = d['narasi_attention_mask'].to(device)\n",
        "    input_mask2 = d['judul_attention_mask'].to(device)\n",
        "\n",
        "    label = d['label'].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "        input_ids1, \n",
        "        input_mask1, \n",
        "        input_ids2, \n",
        "        input_mask2\n",
        "    )\n",
        "\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, label)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == label)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "\n",
        "    loss.backward() \n",
        "    nn.utils.clip_grad_norm(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step() \n",
        "    optimizer.zero_grad()\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples) : \n",
        "  model = model.eval() \n",
        "\n",
        "  losses = []\n",
        "  correct_predictions=0\n",
        "\n",
        "  with torch.no_grad() : \n",
        "    for d in data_loader :\n",
        "      input_ids1 = d['narasi_input_ids'].to(device)\n",
        "      input_ids2 = d['judul_input_ids'].to(device)\n",
        "\n",
        "      input_mask1 = d['narasi_attention_mask'].to(device)\n",
        "      input_mask2 = d['judul_attention_mask'].to(device)\n",
        "\n",
        "      label = d['label'].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids1, \n",
        "        input_mask1, \n",
        "        input_ids2, \n",
        "        input_mask2\n",
        "      )\n",
        "\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      loss = loss_fn(outputs, label)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == label)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD8K_9xfR9re",
        "outputId": "3b97c401-ffcd-4d17-dbd6-04382b742254",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_function, \n",
        "    opt, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_function, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model.bin')\n",
        "    best_accuracy = val_acc\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1773: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.49271536699864044 accuracy 0.8163908589440505\n",
            "Val   loss 0.41279488056898117 accuracy 0.8608490566037735\n",
            "\n",
            "Epoch 2/8\n",
            "----------\n",
            "Train loss 0.4357091244779715 accuracy 0.8381928027318098\n",
            "Val   loss 0.3932666629552841 accuracy 0.8726415094339622\n",
            "\n",
            "Epoch 3/8\n",
            "----------\n",
            "Train loss 0.45868178939118104 accuracy 0.8429209351195167\n",
            "Val   loss 0.40619670173951555 accuracy 0.8655660377358491\n",
            "\n",
            "Epoch 4/8\n",
            "----------\n",
            "Train loss 0.44493169606733723 accuracy 0.8497504596795378\n",
            "Val   loss 0.3992500017796244 accuracy 0.8702830188679245\n",
            "\n",
            "Epoch 5/8\n",
            "----------\n",
            "Train loss 0.42953632696836935 accuracy 0.8481744155503022\n",
            "Val   loss 0.5043437821524483 accuracy 0.7665094339622641\n",
            "\n",
            "Epoch 6/8\n",
            "----------\n",
            "Train loss 0.43514179494701516 accuracy 0.8387181507748884\n",
            "Val   loss 0.3823168969580105 accuracy 0.875\n",
            "\n",
            "Epoch 7/8\n",
            "----------\n",
            "Train loss 0.4622225541026652 accuracy 0.837404780667192\n",
            "Val   loss 0.43377709708043505 accuracy 0.8490566037735848\n",
            "\n",
            "Epoch 8/8\n",
            "----------\n",
            "Train loss 0.4732806594932781 accuracy 0.8289992119779355\n",
            "Val   loss 0.445618847651141 accuracy 0.8443396226415094\n",
            "\n",
            "CPU times: user 5min 13s, sys: 1min 22s, total: 6min 35s\n",
            "Wall time: 6min 37s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp8JaNU92c2r"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      input_ids1 = d['narasi_input_ids'].to(device)\n",
        "      input_ids2 = d['judul_input_ids'].to(device)\n",
        "\n",
        "      input_mask1 = d['narasi_attention_mask'].to(device)\n",
        "      input_mask2 = d['judul_attention_mask'].to(device)\n",
        "\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids1, \n",
        "        input_mask1, \n",
        "        input_ids2, \n",
        "        input_mask2\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(probs)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  return predictions, prediction_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWalW4asXR3j",
        "outputId": "01239b5c-7108-444c-ee0a-9cc3c68f804e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# load best model \n",
        "model.load_state_dict(torch.load('./best_model.bin'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlOnRm11wa2y",
        "outputId": "5e2c1912-0c24-49db-dc52-b4c2a361e05e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "y_pred, y_pred_probs = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1773: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htgbm9SdFQ3-",
        "outputId": "b6ad31b0-ac60-4bf8-b6b9-20b574c07b5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
              "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
              "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
              "        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQNXaB2ezv7U",
        "outputId": "bf53b05d-d827-4343-ee44-2046d4cc6032",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(classification_report(list(test['label']), y_pred))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.57      0.64        60\n",
            "           1       0.94      0.97      0.95       410\n",
            "\n",
            "    accuracy                           0.92       470\n",
            "   macro avg       0.83      0.77      0.79       470\n",
            "weighted avg       0.91      0.92      0.91       470\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hekLp5CuR2e",
        "outputId": "2c43b0dd-c305-40d4-dce6-6dd936093659",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "print(f1_score(list(test['label']), y_pred, average='micro'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9170212765957447\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5sqCcMu6z7o"
      },
      "source": [
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "\n",
        "    # cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    # print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title, fontsize=20)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, fontsize=13)\n",
        "    plt.yticks(tick_marks, classes, fontsize=13)\n",
        "\n",
        "    fmt = '.2f'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label', fontsize=17)\n",
        "    plt.xlabel('Predicted label', fontsize=17)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9OosNAIGFsW",
        "outputId": "bb27837e-d920-4d79-be06-bcb79437ee32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "cnf_matrix = confusion_matrix(test.label.to_list(), y_pred)\n",
        "plt.figure(figsize=(6,6))\n",
        "plot_confusion_matrix(cnf_matrix, classes=['0', '1'], title=\"Confusion matrix\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmjzO51Qob72"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}